{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshavardhank/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/harshavardhank/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware accelerator: mps\n"
     ]
    }
   ],
   "source": [
    "# Set hardware acceralator\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') #Apple Metal Framework for M chips\n",
    "\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "\n",
    "print('Hardware accelerator: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_watermark_processor import WatermarkDetector, WatermarkLogitsProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the watermark algorithm with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DEFAULT CONSTANTS\n",
    "\n",
    "seeding_scheme = \"simple_1\"\n",
    "gamma=0.25\n",
    "delta=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')\n",
    "prompt = (\n",
    "        \"The diamondback terrapin or simply terrapin (Malaclemys terrapin) is a \"\n",
    "        \"species of turtle native to the brackish coastal tidal marshes of the \"\n",
    "        \"Northeastern and southern United States, and in Bermuda.[6] It belongs \"\n",
    "        \"to the monotypic genus Malaclemys. It has one of the largest ranges of \"\n",
    "        \"all turtles in North America, stretching as far south as the Florida Keys \"\n",
    "        \"and as far north as Cape Cod.[7] The name 'terrapin' is derived from the \"\n",
    "        \"Algonquian word torope.[8] It applies to Malaclemys terrapin in both \"\n",
    "        \"British English and American English. The name originally was used by \"\n",
    "        \"early European settlers in North America to describe these brackish-water \"\n",
    "        \"turtles that inhabited neither freshwater habitats nor the sea. It retains \"\n",
    "        \"this primary meaning in American English.[8] In British English, however, \"\n",
    "        \"other semi-aquatic turtle species, such as the red-eared slider, might \"\n",
    "        \"also be called terrapins. The common name refers to the diamond pattern \"\n",
    "        \"on top of its shell (carapace), but the overall pattern and coloration \"\n",
    "        \"vary greatly. The shell is usually wider at the back than in the front, \"\n",
    "        \"and from above it appears wedge-shaped. The shell coloring can vary \"\n",
    "        \"from brown to grey, and its body color can be grey, brown, yellow, \"\n",
    "        \"or white. All have a unique pattern of wiggly, black markings or spots \"\n",
    "        \"on their body and head. The diamondback terrapin has large webbed \"\n",
    "        \"feet.[9] The species is\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_watermark_processor import WatermarkLogitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                               gamma=gamma,\n",
    "                                               delta=delta,\n",
    "                                               seeding_scheme=seeding_scheme,\n",
    "                                               select_green_tokens=True)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1260443d0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = dict(max_new_tokens=50) #keep this at 200 at all times (some reason if < 200 this behaves weird)\n",
    "\n",
    "if use_sampling:\n",
    "    gen_kwargs.update(dict(\n",
    "        do_sample = True, \n",
    "        top_k = 0,\n",
    "        temperature = 0.7\n",
    "    ))\n",
    "else:\n",
    "    gen_kwargs.update(dict(\n",
    "        num_beams = 4\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True).to(device)\n",
    "truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == 200 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_with_watermark = model.generate(**tokd_input, \n",
    "                                       logits_processor=LogitsProcessorList([watermark_processor]), \n",
    "                                       **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1260443d0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(decoded_watermark))\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')\n",
    "\n",
    "#output_text = \"This is not AI related. But can you detect if this has been watermarked?\"#decoded_watermark[0]\n",
    "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=gamma,\n",
    "                                        seeding_scheme=seeding_scheme,\n",
    "                                        device='mps',\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=4.0,\n",
    "                                        normalizers=\"\",\n",
    "                                        ignore_repeated_ngrams=False,\n",
    "                                        select_green_tokens=True)\n",
    "\n",
    "score_dict = watermark_detector.detect(decoded_watermark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_tokens_scored': 513, 'num_green_tokens': 167, 'green_fraction': 0.3255360623781676, 'z_score': 3.951048749993008, 'p_value': 3.8904735617074984e-05, 'z_score_at_T': tensor([-0.5774, -0.8165, -1.0000, -1.1547, -1.2910, -1.4142, -1.5275, -1.6330,\n",
      "        -1.7321, -1.8257, -1.9149, -1.3333, -0.8006, -0.9258, -1.0435, -1.1547,\n",
      "        -1.2603, -1.3608, -1.4570, -1.0328, -1.1339, -0.7385, -0.8427, -0.9428,\n",
      "        -1.0392, -1.1323, -1.2222, -1.3093, -1.3937, -1.4757, -1.1406, -1.2247,\n",
      "        -1.3065, -0.9901, -0.6831, -0.7698, -0.8542, -0.9366, -0.6472, -0.7303,\n",
      "        -0.8115, -0.8909, -0.6163, -0.6963, -0.7746, -0.8513, -0.9264, -1.0000,\n",
      "        -0.7423, -0.8165, -0.8893, -0.9608, -0.7137, -0.4714, -0.5449, -0.6172,\n",
      "        -0.6882, -0.7581, -0.8268, -0.5963, -0.6653, -0.7332, -0.8001, -0.8660,\n",
      "        -0.6445, -0.7107, -0.7759, -0.5601, -0.6255, -0.6901, -0.7537, -0.8165,\n",
      "        -0.8785, -0.9396, -1.0000, -1.0596, -1.1185, -1.1767, -1.2342, -1.2910,\n",
      "        -1.0906, -1.1476, -0.9506, -0.7559, -0.8141, -0.6226, -0.6809, -0.4924,\n",
      "        -0.3060, -0.3651, -0.4237, -0.4815, -0.5388, -0.5955, -0.4146, -0.4714,\n",
      "        -0.5276, -0.5832, -0.4062, -0.4619, -0.5170, -0.5717, -0.6258, -0.6794,\n",
      "        -0.7325, -0.7851, -0.8372, -0.8889, -0.9401, -0.9909, -1.0412, -1.0911,\n",
      "        -1.1406, -0.9733, -1.0229, -1.0721, -0.9074, -0.9567, -1.0056, -1.0541,\n",
      "        -1.1022, -1.1500, -1.1973, -1.2443, -1.2910, -1.3373, -1.3833, -1.4289,\n",
      "        -1.2708, -1.3166, -1.3620, -1.4071, -1.4518, -1.4963, -1.5404, -1.5842,\n",
      "        -1.4305, -1.4744, -1.5181, -1.5614, -1.6045, -1.6473, -1.6898, -1.5396,\n",
      "        -1.5822, -1.4335, -1.4762, -1.5187, -1.5608, -1.4142, -1.4565, -1.4985,\n",
      "        -1.5403, -1.5818, -1.6231, -1.4792, -1.3362, -1.1942, -1.2362, -1.0954,\n",
      "        -1.1375, -1.1794, -1.2210, -1.0820, -1.1237, -1.1651, -1.2063, -1.0690,\n",
      "        -1.1103, -1.1513, -1.1921, -1.2326, -1.0974, -1.1380, -1.1784, -1.2185,\n",
      "        -1.0849, -1.1251, -1.1651, -1.2049, -1.2445, -1.2839, -1.3230, -1.1918,\n",
      "        -1.2310, -1.2700, -1.1399, -1.1790, -1.0499, -1.0890, -1.1279, -1.1667,\n",
      "        -1.2052, -1.0777, -1.1163, -1.1547, -1.0284, -1.0668, -1.1050, -1.1431,\n",
      "        -1.0181, -1.0562, -1.0941, -1.1318, -1.1694, -1.2068, -1.2440, -1.2810,\n",
      "        -1.3179, -1.3546, -1.3911, -1.4275, -1.4637, -1.3419, -1.3781, -1.4142,\n",
      "        -1.4501, -1.4859, -1.5215, -1.5570, -1.5923, -1.6275, -1.6625, -1.5430,\n",
      "        -1.5781, -1.6130, -1.6478, -1.6824, -1.7169, -1.7512, -1.7854, -1.8194,\n",
      "        -1.8534, -1.8871, -1.9208, -1.9543, -1.9877, -1.8712, -1.7552, -1.7889,\n",
      "        -1.8223, -1.8557, -1.7407, -1.7741, -1.8074, -1.6933, -1.7266, -1.7598,\n",
      "        -1.7928, -1.8257, -1.8585, -1.8912, -1.9238, -1.9562, -1.9885, -2.0207,\n",
      "        -2.0528, -2.0848, -2.1166, -2.1483, -2.1800, -2.0688, -1.9581, -1.8477,\n",
      "        -1.8797, -1.9116, -1.9433, -1.9750, -2.0065, -2.0379, -2.0692, -2.1004,\n",
      "        -2.1315, -2.1625, -2.1934, -2.2242, -2.2548, -2.1469, -2.0393, -2.0702,\n",
      "        -2.1010, -1.9941, -1.8876, -1.9185, -1.8126, -1.8435, -1.8744, -1.9052,\n",
      "        -1.8000, -1.6952, -1.7261, -1.7569, -1.7876, -1.6836, -1.7143, -1.7450,\n",
      "        -1.7756, -1.8060, -1.8364, -1.8667, -1.8968, -1.9269, -1.9569, -1.9868,\n",
      "        -2.0166, -2.0463, -2.0759, -2.1054, -2.0035, -2.0331, -2.0625, -1.9612,\n",
      "        -1.8601, -1.7594, -1.6590, -1.5590, -1.4592, -1.4893, -1.5193, -1.5492,\n",
      "        -1.5790, -1.4800, -1.3814, -1.2830, -1.1849, -1.0872, -0.9898, -0.8926,\n",
      "        -0.7958, -0.6992, -0.7299, -0.7605, -0.7910, -0.6950, -0.7255, -0.6299,\n",
      "        -0.6605, -0.6909, -0.5958, -0.6262, -0.5315, -0.5620, -0.4676, -0.3735,\n",
      "        -0.2798, -0.3104, -0.3409, -0.2476, -0.2781, -0.3086, -0.3390, -0.2462,\n",
      "        -0.1536, -0.0614, -0.0919,  0.0000,  0.0917,  0.1831,  0.1524,  0.2434,\n",
      "         0.3343,  0.4248,  0.5152,  0.4842,  0.5742,  0.5432,  0.6329,  0.7223,\n",
      "         0.6913,  0.6603,  0.7494,  0.8382,  0.9267,  1.0150,  0.9839,  1.0719,\n",
      "         1.0407,  1.0097,  1.0973,  1.1847,  1.2719,  1.2407,  1.3276,  1.2964,\n",
      "         1.2653,  1.3518,  1.4381,  1.4069,  1.4929,  1.4618,  1.5475,  1.6330,\n",
      "         1.6018,  1.5707,  1.5396,  1.6247,  1.5937,  1.5628,  1.6475,  1.6166,\n",
      "         1.7011,  1.7853,  1.8694,  1.9532,  2.0369,  2.1204,  2.2036,  2.2866,\n",
      "         2.3695,  2.3381,  2.4207,  2.3893,  2.4716,  2.5538,  2.6357,  2.6042,\n",
      "         2.6859,  2.6545,  2.7359,  2.8172,  2.7857,  2.8667,  2.8352,  2.9160,\n",
      "         2.8846,  2.9651,  3.0455,  3.1256,  3.0941,  3.1740,  3.2538,  3.3333,\n",
      "         3.4127,  3.4919,  3.4602,  3.5392,  3.6180,  3.5863,  3.6649,  3.6332,\n",
      "         3.7115,  3.6799,  3.7580,  3.8360,  3.8043,  3.8820,  3.8504,  3.8188,\n",
      "         3.8963,  3.8648,  3.8333,  3.8019,  3.7706,  3.8477,  3.8164,  3.7852,\n",
      "         3.7540,  3.7229,  3.6919,  3.7687,  3.8453,  3.8142,  3.7833,  3.7524,\n",
      "         3.7216,  3.6908,  3.6602,  3.6296,  3.5990,  3.6751,  3.7510,  3.8268,\n",
      "         3.7962,  3.7656,  3.7352,  3.7048,  3.6745,  3.7498,  3.8251,  3.9001,\n",
      "         3.9751,  3.9446,  3.9143,  3.9890,  4.0635,  4.0331,  4.0028,  3.9726,\n",
      "         4.0468,  4.1210,  4.0907,  4.0605,  4.0304,  4.0003,  3.9703,  3.9404,\n",
      "         3.9106,  3.8808,  3.8510,  3.8213,  3.8949,  3.8653,  3.8357,  3.9090,\n",
      "         3.8795,  3.8500,  3.9231,  3.8936,  3.9665,  3.9371,  3.9077,  3.9804,\n",
      "         3.9510]), 'prediction': False}\n"
     ]
    }
   ],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Gamma and analyzing its impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keep Z threshold = 4.0 score and Delta = 2.0 constant and vary Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's play around with Gamma and see what happens to the Z score in the detector. We'll use the same prompt\n",
    "gamma = 0.25 # now green_list_size = |V| * 0.5 (green list half of vocab size)\n",
    "delta = 4 #Maintaining the same delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_watermark(gamma=0.25, delta=2.0, num_beams=1, gen_kwargs=gen_kwargs):\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                               gamma=gamma,\n",
    "                                               delta=delta,\n",
    "                                               seeding_scheme=seeding_scheme,\n",
    "                                               select_green_tokens=True)\n",
    "    \n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True).to(device)\n",
    "    truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == 200 else False  \n",
    "    \n",
    "    output_with_watermark = model.generate(**tokd_input, \n",
    "                                       logits_processor=LogitsProcessorList([watermark_processor]), \n",
    "                                       **gen_kwargs)\n",
    "    \n",
    "    decoded_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return decoded_watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_watermark(text_to_detect, gamma, z):\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-1.3b')\n",
    "\n",
    "    #output_text = \"This is not AI related. But can you detect if this has been watermarked?\"#decoded_watermark[0]\n",
    "    watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=gamma,\n",
    "                                        seeding_scheme=seeding_scheme,\n",
    "                                        device='mps',\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=z,\n",
    "                                        normalizers=\"\",\n",
    "                                        ignore_repeated_ngrams=False,\n",
    "                                        select_green_tokens=True)\n",
    "    \n",
    "    score_dict = watermark_detector.detect(text_to_detect)\n",
    "\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshavardhank/Desktop/Code/CS222 NLP/lm-watermarking/tsar_periments.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m decoded_watermark \u001b[39m=\u001b[39m generate_watermark(gamma, delta)\n",
      "\u001b[1;32m/Users/harshavardhank/Desktop/Code/CS222 NLP/lm-watermarking/tsar_periments.ipynb Cell 23\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_watermark\u001b[39m(gamma\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m, delta\u001b[39m=\u001b[39m\u001b[39m2.0\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     watermark_processor \u001b[39m=\u001b[39m WatermarkLogitsProcessor(vocab\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(tokenizer\u001b[39m.\u001b[39mget_vocab()\u001b[39m.\u001b[39mvalues()),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                gamma\u001b[39m=\u001b[39mgamma,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                delta\u001b[39m=\u001b[39mdelta,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                                seeding_scheme\u001b[39m=\u001b[39mseeding_scheme,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                select_green_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     output_with_watermark \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokd_input, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                        logits_processor\u001b[39m=\u001b[39;49mLogitsProcessorList([watermark_processor]), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                        \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgen_kwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     decoded_watermark \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(output_with_watermark, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m decoded_watermark\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1720\u001b[0m         input_ids,\n\u001b[1;32m   1721\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1722\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1723\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1724\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1725\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1726\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1727\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1728\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1729\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1733\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2803\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2804\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2805\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2806\u001b[0m )\n\u001b[1;32m   2808\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1521\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/opt/modeling_opt.py:879\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    878\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 879\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    880\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    881\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    882\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    883\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    884\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    885\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    886\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    887\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    888\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    889\u001b[0m )\n\u001b[1;32m    891\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    893\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1521\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/opt/modeling_opt.py:645\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    635\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    636\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    637\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m         use_cache,\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 645\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    646\u001b[0m         hidden_states,\n\u001b[1;32m    647\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    648\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    649\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    650\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    651\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    656\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1521\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/opt/modeling_opt.py:299\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    296\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    298\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    300\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    301\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    302\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    303\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    304\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    306\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    307\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1521\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/opt/modeling_opt.py:194\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention mask should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39m1\u001b[39m,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39msrc_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    193\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(\n\u001b[0;32m--> 194\u001b[0m         attn_weights, torch\u001b[39m.\u001b[39;49mtensor(torch\u001b[39m.\u001b[39;49mfinfo(attn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mmin, device\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[1;32m    198\u001b[0m \u001b[39m# upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoded_watermark = generate_watermark(gamma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshavardhank/Desktop/Code/CS222 NLP/lm-watermarking/tsar_periments.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m score_dict \u001b[39m=\u001b[39m detect_watermark(decoded_watermark, gamma, delta)\n",
      "\u001b[1;32m/Users/harshavardhank/Desktop/Code/CS222 NLP/lm-watermarking/tsar_periments.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#output_text = \"This is not AI related. But can you detect if this has been watermarked?\"#decoded_watermark[0]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m watermark_detector \u001b[39m=\u001b[39m WatermarkDetector(vocab\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(tokenizer\u001b[39m.\u001b[39mget_vocab()\u001b[39m.\u001b[39mvalues()),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                     gamma\u001b[39m=\u001b[39mgamma,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                     seeding_scheme\u001b[39m=\u001b[39mseeding_scheme,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                     ignore_repeated_ngrams\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                     select_green_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m score_dict \u001b[39m=\u001b[39m watermark_detector\u001b[39m.\u001b[39;49mdetect(text_to_detect)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshavardhank/Desktop/Code/CS222%20NLP/lm-watermarking/tsar_periments.ipynb#X55sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m score_dict\n",
      "File \u001b[0;32m~/Desktop/Code/CS222 NLP/lm-watermarking/extended_watermark_processor.py:582\u001b[0m, in \u001b[0;36mWatermarkDetector.detect\u001b[0;34m(self, text, tokenized_text, window_size, window_stride, return_prediction, return_scores, z_threshold, convert_to_float, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     output_dict\u001b[39m.\u001b[39mupdate(score_dict)\n\u001b[1;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     score_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score_sequence(tokenized_text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    583\u001b[0m \u001b[39mif\u001b[39;00m return_scores:\n\u001b[1;32m    584\u001b[0m     output_dict\u001b[39m.\u001b[39mupdate(score_dict)\n",
      "File \u001b[0;32m~/Desktop/Code/CS222 NLP/lm-watermarking/extended_watermark_processor.py:391\u001b[0m, in \u001b[0;36mWatermarkDetector._score_sequence\u001b[0;34m(self, input_ids, return_num_tokens_scored, return_num_green_tokens, return_green_fraction, return_green_token_mask, return_z_score, return_z_at_T, return_p_value)\u001b[0m\n\u001b[1;32m    389\u001b[0m     score_dict\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(green_fraction\u001b[39m=\u001b[39m(green_token_count \u001b[39m/\u001b[39m num_tokens_scored)))\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m return_z_score:\n\u001b[0;32m--> 391\u001b[0m     score_dict\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(z_score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_z_score(green_token_count, num_tokens_scored)))\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m return_p_value:\n\u001b[1;32m    393\u001b[0m     z_score \u001b[39m=\u001b[39m score_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mz_score\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Code/CS222 NLP/lm-watermarking/extended_watermark_processor.py:291\u001b[0m, in \u001b[0;36mWatermarkDetector._compute_z_score\u001b[0;34m(self, observed_count, T)\u001b[0m\n\u001b[1;32m    289\u001b[0m numer \u001b[39m=\u001b[39m observed_count \u001b[39m-\u001b[39m expected_count \u001b[39m*\u001b[39m T\n\u001b[1;32m    290\u001b[0m denom \u001b[39m=\u001b[39m sqrt(T \u001b[39m*\u001b[39m expected_count \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m expected_count))\n\u001b[0;32m--> 291\u001b[0m z \u001b[39m=\u001b[39m numer \u001b[39m/\u001b[39;49m denom\n\u001b[1;32m    292\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "score_dict = detect_watermark(decoded_watermark, gamma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_tokens_scored': 513,\n",
       " 'num_green_tokens': 412,\n",
       " 'green_fraction': 0.8031189083820662,\n",
       " 'z_score': 2.77847944354347,\n",
       " 'p_value': 0.0027306984391784945,\n",
       " 'z_score_at_T': tensor([-1.7321, -0.8165, -1.6667, -1.1547, -1.8074, -2.3570, -1.9640, -2.4495,\n",
       "         -2.1170, -2.5560, -2.2630, -2.0000, -1.7614, -2.1602, -2.5342, -2.3094,\n",
       "         -2.6605, -2.4495, -2.2517, -2.0656, -2.3938, -2.2156, -2.0466, -2.3570,\n",
       "         -2.1939, -2.0381, -2.3333, -2.1822, -2.4659, -2.3190, -2.1776, -2.0412,\n",
       "         -1.9096, -1.7823, -1.6590, -1.9245, -2.1831, -2.0605, -1.9415, -1.8257,\n",
       "         -1.7132, -1.6036, -1.4968, -1.3926, -1.2910, -1.5323, -1.4317, -1.6667,\n",
       "         -1.5671, -1.4697, -1.3744, -1.2810, -1.1896, -1.0999, -1.3234, -1.2344,\n",
       "         -1.1471, -1.0613, -1.2778, -1.1926, -1.1088, -1.0265, -0.9456, -0.8660,\n",
       "         -0.7877, -0.7107, -0.6348, -0.5601, -0.4865, -0.6901, -0.8907, -0.8165,\n",
       "         -0.7433, -0.9396, -0.8667, -0.7947, -0.7237, -0.6537, -0.5846, -0.7746,\n",
       "         -0.7057, -0.6376, -0.5704, -0.5040, -0.4384, -0.3735, -0.3095, -0.2462,\n",
       "         -0.1836, -0.1217, -0.3026, -0.4815, -0.4191, -0.3573, -0.2962, -0.2357,\n",
       "         -0.1759, -0.3499, -0.2901, -0.2309, -0.1723, -0.1143, -0.0569, -0.2265,\n",
       "         -0.3944, -0.3365, -0.5023, -0.4444, -0.3871, -0.5505, -0.4932, -0.6547,\n",
       "         -0.8147, -0.7570, -0.6999, -0.6433, -0.5871, -0.5315, -0.6880, -0.8433,\n",
       "         -0.7873, -0.9409, -1.0932, -1.2443, -1.1877, -1.1316, -1.2808, -1.2247,\n",
       "         -1.1692, -1.3166, -1.2611, -1.2060, -1.3517, -1.4963, -1.4410, -1.3862,\n",
       "         -1.3318, -1.2778, -1.2243, -1.1711, -1.1183, -1.0659, -1.0139, -0.9623,\n",
       "         -1.1028, -1.0512, -1.0000, -1.1390, -1.0879, -1.0371, -0.9867, -1.1239,\n",
       "         -1.2603, -1.3957, -1.5303, -1.4792, -1.4284, -1.3779, -1.3278, -1.2780,\n",
       "         -1.2285, -1.3608, -1.3114, -1.2623, -1.3933, -1.3443, -1.2956, -1.2472,\n",
       "         -1.1991, -1.3284, -1.4570, -1.4087, -1.3607, -1.3131, -1.2657, -1.2185,\n",
       "         -1.1717, -1.1251, -1.0788, -1.0328, -0.9870, -0.9415, -0.8963, -0.8513,\n",
       "         -0.8065, -0.7620, -0.7177, -0.8422, -0.7979, -0.7539, -0.7102, -0.6667,\n",
       "         -0.7896, -0.7461, -0.7029, -0.6598, -0.6170, -0.5744, -0.5321, -0.6532,\n",
       "         -0.6108, -0.5687, -0.6889, -0.8085, -0.7662, -0.7241, -0.6822, -0.6405,\n",
       "         -0.5990, -0.5578, -0.5167, -0.4758, -0.5934, -0.5525, -0.5119, -0.4714,\n",
       "         -0.5879, -0.5474, -0.6632, -0.7785, -0.8932, -1.0075, -0.9666, -0.9258,\n",
       "         -0.8853, -0.8449, -0.8047, -0.7647, -0.7249, -0.6852, -0.6458, -0.7581,\n",
       "         -0.7186, -0.6794, -0.6403, -0.7516, -0.8626, -0.8233, -0.7843, -0.7454,\n",
       "         -0.7066, -0.6680, -0.6296, -0.7392, -0.7008, -0.6626, -0.6245, -0.7332,\n",
       "         -0.6952, -0.8033, -0.7653, -0.7274, -0.6897, -0.6521, -0.6146, -0.5774,\n",
       "         -0.6843, -0.6470, -0.6099, -0.5729, -0.5361, -0.4994, -0.4628, -0.4264,\n",
       "         -0.3901, -0.4956, -0.4593, -0.4232, -0.3872, -0.3514, -0.3156, -0.4201,\n",
       "         -0.5241, -0.4883, -0.4526, -0.5560, -0.6591, -0.6233, -0.5876, -0.5521,\n",
       "         -0.5166, -0.4813, -0.4462, -0.4111, -0.3762, -0.3414, -0.3067, -0.2722,\n",
       "         -0.2377, -0.2034, -0.1692, -0.2703, -0.2361, -0.2020, -0.3025, -0.2685,\n",
       "         -0.2345, -0.2007, -0.1669, -0.2667, -0.2329, -0.3322, -0.2985, -0.3974,\n",
       "         -0.3636, -0.4621, -0.4284, -0.3948, -0.3613, -0.3279, -0.2946, -0.2615,\n",
       "         -0.2284, -0.1955, -0.1627, -0.1299, -0.0973, -0.0648, -0.0323,  0.0000,\n",
       "          0.0322,  0.0643,  0.0964,  0.1283,  0.1601,  0.1919,  0.2235,  0.2550,\n",
       "          0.2865,  0.3178,  0.3491,  0.3802,  0.4113,  0.4423,  0.4732,  0.5040,\n",
       "          0.4089,  0.4397,  0.4704,  0.5010,  0.5315,  0.5620,  0.5923,  0.6226,\n",
       "          0.6528,  0.6828,  0.7129,  0.7428,  0.7726,  0.8024,  0.8321,  0.8616,\n",
       "          0.8911,  0.9206,  0.9499,  0.8568,  0.8861,  0.9154,  0.9446,  0.9737,\n",
       "          1.0028,  0.9103,  0.8182,  0.8473,  0.8764,  0.9054,  0.9343,  0.9631,\n",
       "          0.9918,  1.0205,  1.0491,  1.0776,  1.1061,  1.1345,  1.1628,  1.1910,\n",
       "          1.2191,  1.2472,  1.2752,  1.3032,  1.3310,  1.3588,  1.3866,  1.2964,\n",
       "          1.3241,  1.3518,  1.2620,  1.2897,  1.3173,  1.3448,  1.3723,  1.3997,\n",
       "          1.4270,  1.3380,  1.3653,  1.2766,  1.3039,  1.3312,  1.3585,  1.3856,\n",
       "          1.2974,  1.3246,  1.3517,  1.3788,  1.4057,  1.3181,  1.3451,  1.2577,\n",
       "          1.2847,  1.3116,  1.2246,  1.2515,  1.2784,  1.3053,  1.3320,  1.3587,\n",
       "          1.2723,  1.2990,  1.3257,  1.3522,  1.3788,  1.2928,  1.3194,  1.3459,\n",
       "          1.3723,  1.3986,  1.4249,  1.4512,  1.4774,  1.5035,  1.5295,  1.4444,\n",
       "          1.4705,  1.3857,  1.4118,  1.4378,  1.4638,  1.4897,  1.5155,  1.5413,\n",
       "          1.5671,  1.5928,  1.6184,  1.6440,  1.6695,  1.6950,  1.6112,  1.6366,\n",
       "          1.6621,  1.6874,  1.7127,  1.7380,  1.7632,  1.7884,  1.8135,  1.8385,\n",
       "          1.8635,  1.8884,  1.9133,  1.9382,  1.9630,  1.8803,  1.9051,  1.9298,\n",
       "          1.9545,  1.9791,  2.0037,  2.0283,  2.0528,  2.0772,  2.1016,  2.1260,\n",
       "          2.1503,  2.1745,  2.1987,  2.2229,  2.2470,  2.2710,  2.2950,  2.3190,\n",
       "          2.3429,  2.3668,  2.3906,  2.4144,  2.4381,  2.4618,  2.4854,  2.4045,\n",
       "          2.4281,  2.4517,  2.4753,  2.4988,  2.4182,  2.4418,  2.4652,  2.4887,\n",
       "          2.5121,  2.5354,  2.5587,  2.5820,  2.6052,  2.6284,  2.6515,  2.6746,\n",
       "          2.6976,  2.7206,  2.7436,  2.7665,  2.7894,  2.8122,  2.7328,  2.7557,\n",
       "          2.7785]),\n",
       " 'prediction': True,\n",
       " 'confidence': 0.9972693015608215}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Gamma and high Delta\n",
    "\n",
    "#### The paper states that we can achieve a very strong watermark from a low gamma value (smaller greenlist size) and a higher bias (delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.15\n",
    "delta = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_watermark = generate_watermark(gamma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = detect_watermark(decoded_watermark, gamma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_tokens_scored': 513,\n",
       " 'num_green_tokens': 221,\n",
       " 'green_fraction': 0.43079922027290446,\n",
       " 'z_score': 17.81145867798986,\n",
       " 'p_value': 2.8794615316557132e-71,\n",
       " 'z_score_at_T': tensor([-4.2008e-01, -5.9409e-01, -7.2761e-01, -8.4017e-01, -9.3934e-01,\n",
       "         -1.0290e+00, -1.1114e+00, -1.1882e+00, -1.2603e+00, -1.3284e+00,\n",
       "         -1.3933e+00, -6.4676e-01, -7.3790e-01, -8.2333e-01, -9.0388e-01,\n",
       "         -9.8020e-01, -1.0528e+00, -1.1222e+00, -1.1886e+00, -1.2524e+00,\n",
       "         -1.3139e+00, -7.7621e-01, -8.4674e-01, -9.1466e-01, -9.8020e-01,\n",
       "         -1.0435e+00, -1.1049e+00, -1.1644e+00, -1.2221e+00, -1.2783e+00,\n",
       "         -8.2994e-01, -8.9113e-01, -9.5065e-01, -1.0086e+00, -1.0651e+00,\n",
       "         -1.1202e+00, -1.1740e+00, -1.2266e+00, -8.2963e-01, -8.8561e-01,\n",
       "         -9.4035e-01, -9.9391e-01, -1.0463e+00, -1.0977e+00, -1.1481e+00,\n",
       "         -1.1975e+00, -1.2459e+00, -1.2935e+00, -9.4019e-01, -9.9015e-01,\n",
       "         -1.0392e+00, -1.0874e+00, -7.5014e-01, -4.1922e-01, -4.7203e-01,\n",
       "         -5.2394e-01, -5.7496e-01, -6.2514e-01, -6.7451e-01, -3.6155e-01,\n",
       "         -4.1236e-01, -4.6237e-01, -5.1161e-01, -5.6011e-01, -6.0789e-01,\n",
       "         -6.5498e-01, -7.0139e-01, -7.4716e-01, -7.9230e-01, -8.3683e-01,\n",
       "         -8.8077e-01, -9.2414e-01, -9.6695e-01, -1.0092e+00, -1.0510e+00,\n",
       "         -1.0922e+00, -1.1330e+00, -1.1733e+00, -1.2131e+00, -1.2524e+00,\n",
       "         -9.8020e-01, -1.0206e+00, -7.5313e-01, -7.9447e-01, -8.3535e-01,\n",
       "         -8.7578e-01, -9.1577e-01, -6.5679e-01, -6.9762e-01, -7.3801e-01,\n",
       "         -7.7798e-01, -8.1754e-01, -8.5669e-01, -8.9545e-01, -9.3383e-01,\n",
       "         -9.7183e-01, -1.0095e+00, -1.0467e+00, -8.0218e-01, -8.4017e-01,\n",
       "         -8.7780e-01, -9.1508e-01, -9.5202e-01, -9.8862e-01, -1.0249e+00,\n",
       "         -1.0609e+00, -1.0965e+00, -1.1318e+00, -1.1669e+00, -1.2016e+00,\n",
       "         -1.2361e+00, -1.2702e+00, -1.3041e+00, -1.0754e+00, -1.1099e+00,\n",
       "         -1.1441e+00, -9.1914e-01, -9.5391e-01, -9.8840e-01, -1.0226e+00,\n",
       "         -1.0566e+00, -1.0903e+00, -1.1237e+00, -1.1569e+00, -1.1898e+00,\n",
       "         -1.2225e+00, -1.2550e+00, -1.2872e+00, -1.0726e+00, -1.1053e+00,\n",
       "         -1.1378e+00, -1.1700e+00, -1.2021e+00, -1.2339e+00, -1.2654e+00,\n",
       "         -1.2968e+00, -1.0887e+00, -1.1205e+00, -1.1521e+00, -1.1835e+00,\n",
       "         -1.2146e+00, -1.2456e+00, -1.2764e+00, -1.0735e+00, -1.1047e+00,\n",
       "         -1.1357e+00, -1.1665e+00, -1.1971e+00, -1.2275e+00, -1.2577e+00,\n",
       "         -1.2877e+00, -1.3175e+00, -1.3472e+00, -1.3766e+00, -1.4059e+00,\n",
       "         -1.4350e+00, -1.2405e+00, -1.2700e+00, -1.2993e+00, -1.1070e+00,\n",
       "         -1.1367e+00, -1.1662e+00, -1.1955e+00, -1.0060e+00, -1.0356e+00,\n",
       "         -1.0651e+00, -1.0944e+00, -9.0749e-01, -9.3711e-01, -9.6657e-01,\n",
       "         -9.9586e-01, -1.0250e+00, -8.4104e-01, -8.7047e-01, -8.9974e-01,\n",
       "         -9.2884e-01, -7.4729e-01, -7.7667e-01, -8.0590e-01, -8.3497e-01,\n",
       "         -8.6388e-01, -8.9264e-01, -9.2125e-01, -9.4972e-01, -9.7803e-01,\n",
       "         -1.0062e+00, -8.2943e-01, -8.5786e-01, -8.8614e-01, -9.1428e-01,\n",
       "         -9.4228e-01, -9.7014e-01, -9.9786e-01, -1.0254e+00, -1.0529e+00,\n",
       "         -1.0802e+00, -9.0787e-01, -9.3543e-01, -9.6285e-01, -9.9015e-01,\n",
       "         -8.1978e-01, -8.4730e-01, -8.7470e-01, -9.0196e-01, -9.2910e-01,\n",
       "         -9.5611e-01, -9.8299e-01, -1.0098e+00, -1.0364e+00, -1.0629e+00,\n",
       "         -1.0893e+00, -1.1156e+00, -1.1418e+00, -9.7636e-01, -1.0027e+00,\n",
       "         -1.0290e+00, -1.0551e+00, -1.0812e+00, -1.1071e+00, -1.1329e+00,\n",
       "         -1.1586e+00, -1.1842e+00, -1.2096e+00, -1.2350e+00, -1.2603e+00,\n",
       "         -1.2854e+00, -1.3105e+00, -1.3354e+00, -1.3602e+00, -1.3850e+00,\n",
       "         -1.4096e+00, -1.4342e+00, -1.4586e+00, -1.4829e+00, -1.5072e+00,\n",
       "         -1.5313e+00, -1.5554e+00, -1.3978e+00, -1.2409e+00, -1.2654e+00,\n",
       "         -1.2899e+00, -1.3142e+00, -1.3384e+00, -1.3626e+00, -1.3866e+00,\n",
       "         -1.2320e+00, -1.2563e+00, -1.2804e+00, -1.3045e+00, -1.3284e+00,\n",
       "         -1.3523e+00, -1.3761e+00, -1.3998e+00, -1.4234e+00, -1.4469e+00,\n",
       "         -1.4703e+00, -1.4936e+00, -1.5169e+00, -1.5401e+00, -1.5632e+00,\n",
       "         -1.5862e+00, -1.6091e+00, -1.4592e+00, -1.4823e+00, -1.5053e+00,\n",
       "         -1.5282e+00, -1.5511e+00, -1.5739e+00, -1.5965e+00, -1.6191e+00,\n",
       "         -1.6417e+00, -1.6641e+00, -1.6865e+00, -1.7088e+00, -1.7310e+00,\n",
       "         -1.7532e+00, -1.7752e+00, -1.7972e+00, -1.6515e+00, -1.6737e+00,\n",
       "         -1.6957e+00, -1.5510e+00, -1.5732e+00, -1.5954e+00, -1.6174e+00,\n",
       "         -1.6394e+00, -1.6614e+00, -1.6833e+00, -1.7050e+00, -1.7268e+00,\n",
       "         -1.7484e+00, -1.7700e+00, -1.7915e+00, -1.6497e+00, -1.6713e+00,\n",
       "         -1.6929e+00, -1.7144e+00, -1.7359e+00, -1.7573e+00, -1.7786e+00,\n",
       "         -1.7999e+00, -1.8210e+00, -1.8422e+00, -1.8632e+00, -1.8842e+00,\n",
       "         -1.9052e+00, -1.9260e+00, -1.9468e+00, -1.9676e+00, -1.9883e+00,\n",
       "         -2.0089e+00, -1.8709e+00, -1.8916e+00, -1.7543e+00, -1.6174e+00,\n",
       "         -1.6385e+00, -1.5022e+00, -1.3663e+00, -1.3877e+00, -1.4090e+00,\n",
       "         -1.2739e+00, -1.2954e+00, -1.1609e+00, -1.0269e+00, -8.9325e-01,\n",
       "         -7.6003e-01, -6.2723e-01, -4.9483e-01, -5.1724e-01, -3.8541e-01,\n",
       "         -2.5399e-01, -1.2296e-01,  7.6734e-03,  1.3792e-01,  2.6777e-01,\n",
       "          3.9724e-01,  5.2632e-01,  6.5502e-01,  6.3124e-01,  7.5941e-01,\n",
       "          7.3555e-01,  8.6319e-01,  9.9046e-01,  1.1174e+00,  1.2439e+00,\n",
       "          1.3701e+00,  1.4959e+00,  1.6214e+00,  1.7465e+00,  1.8712e+00,\n",
       "          1.9956e+00,  2.1196e+00,  2.2433e+00,  2.3667e+00,  2.4897e+00,\n",
       "          2.6124e+00,  2.7347e+00,  2.8567e+00,  2.9783e+00,  3.0997e+00,\n",
       "          3.2206e+00,  3.3413e+00,  3.4616e+00,  3.4349e+00,  3.5548e+00,\n",
       "          3.6743e+00,  3.7936e+00,  3.9125e+00,  4.0311e+00,  4.1494e+00,\n",
       "          4.2674e+00,  4.3851e+00,  4.5025e+00,  4.6196e+00,  4.7363e+00,\n",
       "          4.8528e+00,  4.9689e+00,  5.0848e+00,  5.2004e+00,  5.3156e+00,\n",
       "          5.4306e+00,  5.5453e+00,  5.6597e+00,  5.7738e+00,  5.8876e+00,\n",
       "          6.0011e+00,  5.9720e+00,  6.0852e+00,  6.1980e+00,  6.3106e+00,\n",
       "          6.4229e+00,  6.5350e+00,  6.6467e+00,  6.7582e+00,  6.8694e+00,\n",
       "          6.9804e+00,  7.0911e+00,  7.2015e+00,  7.3116e+00,  7.4215e+00,\n",
       "          7.5311e+00,  7.6405e+00,  7.7495e+00,  7.8584e+00,  7.9670e+00,\n",
       "          8.0753e+00,  8.1834e+00,  8.2912e+00,  8.3987e+00,  8.3677e+00,\n",
       "          8.4750e+00,  8.5820e+00,  8.6887e+00,  8.7952e+00,  8.9014e+00,\n",
       "          9.0075e+00,  9.1132e+00,  9.2187e+00,  9.3240e+00,  9.4291e+00,\n",
       "          9.5339e+00,  9.6385e+00,  9.7428e+00,  9.8469e+00,  9.9508e+00,\n",
       "          1.0054e+01,  1.0158e+01,  1.0261e+01,  1.0364e+01,  1.0467e+01,\n",
       "          1.0569e+01,  1.0672e+01,  1.0639e+01,  1.0741e+01,  1.0843e+01,\n",
       "          1.0944e+01,  1.1046e+01,  1.1147e+01,  1.1248e+01,  1.1348e+01,\n",
       "          1.1449e+01,  1.1549e+01,  1.1649e+01,  1.1749e+01,  1.1849e+01,\n",
       "          1.1948e+01,  1.2047e+01,  1.2146e+01,  1.2245e+01,  1.2344e+01,\n",
       "          1.2442e+01,  1.2540e+01,  1.2638e+01,  1.2736e+01,  1.2834e+01,\n",
       "          1.2800e+01,  1.2897e+01,  1.2995e+01,  1.3092e+01,  1.3188e+01,\n",
       "          1.3285e+01,  1.3381e+01,  1.3477e+01,  1.3573e+01,  1.3669e+01,\n",
       "          1.3765e+01,  1.3860e+01,  1.3955e+01,  1.4050e+01,  1.4145e+01,\n",
       "          1.4240e+01,  1.4334e+01,  1.4429e+01,  1.4523e+01,  1.4617e+01,\n",
       "          1.4710e+01,  1.4804e+01,  1.4897e+01,  1.4863e+01,  1.4956e+01,\n",
       "          1.5049e+01,  1.5142e+01,  1.5234e+01,  1.5327e+01,  1.5419e+01,\n",
       "          1.5511e+01,  1.5603e+01,  1.5695e+01,  1.5786e+01,  1.5878e+01,\n",
       "          1.5969e+01,  1.6060e+01,  1.6151e+01,  1.6242e+01,  1.6332e+01,\n",
       "          1.6423e+01,  1.6513e+01,  1.6603e+01,  1.6693e+01,  1.6783e+01,\n",
       "          1.6872e+01,  1.6837e+01,  1.6926e+01,  1.7015e+01,  1.7105e+01,\n",
       "          1.7193e+01,  1.7282e+01,  1.7371e+01,  1.7459e+01,  1.7548e+01,\n",
       "          1.7636e+01,  1.7724e+01,  1.7811e+01]),\n",
       " 'prediction': True,\n",
       " 'confidence': 1.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict #Observation - a very high Z score indicating a stronger watermark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maintain the low Gamma and high Delta, and now use Beam Search and validate the author's claim of Z score increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z score increases when beam search is used. From 2 beams to 4 beams, the z score jumped from 13.x to 17.x indicating a higher watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_watermark = generate_watermark(gamma, delta, num_beams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = detect_watermark(decoded_watermark, gamma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_tokens_scored': 363,\n",
       " 'num_green_tokens': 78,\n",
       " 'green_fraction': 0.21487603305785125,\n",
       " 'z_score': 3.461644830064026,\n",
       " 'p_value': 0.00026844258081788394,\n",
       " 'z_score_at_T': tensor([-0.4201, -0.5941, -0.7276, -0.8402, -0.9393, -1.0290, -1.1114, -1.1882,\n",
       "         -1.2603, -1.3284, -1.3933, -0.6468, -0.7379, -0.8233, -0.9039, -0.9802,\n",
       "         -1.0528, -1.1222, -1.1886, -1.2524, -1.3139, -0.7762, -0.8467, -0.9147,\n",
       "         -0.9802, -1.0435, -1.1049, -1.1644, -1.2221, -1.2783, -0.8299, -0.8911,\n",
       "         -0.9507, -1.0086, -1.0651, -1.1202, -1.1740, -1.2266, -0.8296, -0.8856,\n",
       "         -0.9404, -0.9939, -1.0463, -1.0977, -1.1481, -1.1975, -1.2459, -1.2935,\n",
       "         -0.9402, -0.9901, -1.0392, -1.0874, -0.7501, -0.4192, -0.4720, -0.5239,\n",
       "         -0.5750, -0.6251, -0.6745, -0.3616, -0.4124, -0.4624, -0.5116, -0.5601,\n",
       "         -0.6079, -0.6550, -0.7014, -0.7472, -0.7923, -0.8368, -0.8808, -0.9241,\n",
       "         -0.9670, -1.0092, -1.0510, -1.0922, -1.1330, -1.1733, -1.2131, -1.2524,\n",
       "         -0.9802, -1.0206, -0.7531, -0.7945, -0.8353, -0.8758, -0.9158, -0.6568,\n",
       "         -0.6976, -0.7380, -0.7780, -0.8175, -0.8567, -0.8955, -0.9338, -0.9718,\n",
       "         -1.0095, -1.0467, -0.8022, -0.8402, -0.8778, -0.9151, -0.9520, -0.9886,\n",
       "         -1.0249, -1.0609, -1.0965, -1.1318, -1.1669, -1.2016, -1.2361, -1.2702,\n",
       "         -1.3041, -1.0754, -1.1099, -1.1441, -0.9191, -0.9539, -0.9884, -1.0226,\n",
       "         -1.0566, -1.0903, -1.1237, -1.1569, -1.1898, -1.2225, -1.2550, -1.2872,\n",
       "         -1.0726, -1.1053, -1.1378, -1.1700, -1.2021, -1.2339, -1.2654, -1.2968,\n",
       "         -1.0887, -1.1205, -1.1521, -1.1835, -1.2146, -1.2456, -1.2764, -1.0735,\n",
       "         -1.1047, -1.1357, -1.1665, -1.1971, -1.2275, -1.2577, -1.2877, -1.3175,\n",
       "         -1.3472, -1.3766, -1.4059, -1.4350, -1.2405, -1.2700, -1.2993, -1.1070,\n",
       "         -1.1367, -1.1662, -1.1955, -1.0060, -1.0356, -1.0651, -1.0944, -0.9075,\n",
       "         -0.9371, -0.9666, -0.9959, -1.0250, -0.8410, -0.8705, -0.8997, -0.9288,\n",
       "         -0.7473, -0.7767, -0.8059, -0.8350, -0.8639, -0.8926, -0.9213, -0.9497,\n",
       "         -0.9780, -1.0062, -0.8294, -0.8579, -0.8861, -0.9143, -0.9423, -0.9701,\n",
       "         -0.9979, -1.0254, -1.0529, -1.0802, -0.9079, -0.9354, -0.9629, -0.9901,\n",
       "         -0.8198, -0.8473, -0.8747, -0.9020, -0.9291, -0.9561, -0.9830, -1.0098,\n",
       "         -1.0364, -1.0629, -1.0893, -1.1156, -1.1418, -0.9764, -1.0027, -1.0290,\n",
       "         -1.0551, -1.0812, -1.1071, -1.1329, -1.1586, -1.1842, -1.2096, -1.2350,\n",
       "         -1.2603, -1.2854, -1.3105, -1.3354, -1.3602, -1.3850, -1.4096, -1.4342,\n",
       "         -1.4586, -1.4829, -1.5072, -1.5313, -1.5554, -1.3978, -1.2409, -1.2654,\n",
       "         -1.2899, -1.3142, -1.3384, -1.3626, -1.3866, -1.2320, -1.2563, -1.2804,\n",
       "         -1.3045, -1.3284, -1.3523, -1.3761, -1.3998, -1.4234, -1.4469, -1.4703,\n",
       "         -1.4936, -1.5169, -1.5401, -1.5632, -1.5862, -1.6091, -1.4592, -1.4823,\n",
       "         -1.5053, -1.5282, -1.5511, -1.5739, -1.5965, -1.6191, -1.6417, -1.6641,\n",
       "         -1.6865, -1.7088, -1.7310, -1.7532, -1.7752, -1.7972, -1.6515, -1.6737,\n",
       "         -1.6957, -1.5510, -1.5732, -1.5954, -1.6174, -1.6394, -1.6614, -1.6833,\n",
       "         -1.7050, -1.7268, -1.7484, -1.7700, -1.7915, -1.6497, -1.6713, -1.6929,\n",
       "         -1.7144, -1.7359, -1.7573, -1.7786, -1.7999, -1.8210, -1.8422, -1.8632,\n",
       "         -1.8842, -1.9052, -1.9260, -1.9468, -1.9676, -1.9883, -2.0089, -1.8709,\n",
       "         -1.8916, -1.7543, -1.6174, -1.6385, -1.5022, -1.3663, -1.3877, -1.4090,\n",
       "         -1.2739, -1.2954, -1.1609, -1.0269, -0.8932, -0.7600, -0.6272, -0.4948,\n",
       "         -0.5172, -0.3854, -0.2540, -0.1230,  0.0077,  0.1379,  0.2678,  0.3972,\n",
       "          0.5263,  0.6550,  0.6312,  0.7594,  0.7355,  0.8632,  0.9905,  1.1174,\n",
       "          1.2439,  1.3701,  1.4959,  1.6214,  1.7465,  1.8712,  1.9956,  2.1196,\n",
       "          2.2433,  2.3667,  2.4897,  2.6124,  2.7347,  2.8567,  2.9783,  3.0997,\n",
       "          3.2206,  3.3413,  3.4616]),\n",
       " 'prediction': False}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's keep delta at 5 amd vary Gamma from 0.1 to 0.9 (0.1, 0.25, 0.5, 0.75, 0.9) on a 4 way beam search\n",
    "#### While also varying token size T (max_new_tokens) from 50 to 200 in steps of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_z_beam(delta, gammas, num_beams=4):\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for max_new_tokens in range(50, 201, 50):\n",
    "\n",
    "        gen_kwargs = dict(max_new_tokens=max_new_tokens)\n",
    "\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams = 4\n",
    "        ))\n",
    "\n",
    "        for gamma in gammas:\n",
    "            score_dicts = {}\n",
    "            \n",
    "            decoded_watermark = generate_watermark(gamma=gamma, delta=delta, num_beams=num_beams, gen_kwargs=gen_kwargs)\n",
    "            score_dict = detect_watermark(decoded_watermark, gamma, delta)\n",
    "\n",
    "            score_dicts['gamma'] = gamma\n",
    "            score_dicts['token_size'] = max_new_tokens\n",
    "            score_dicts['delta'] = delta\n",
    "            score_dicts['z_score'] = score_dict['z_score']\n",
    "            score_dicts['p_value'] = score_dict['p_value']\n",
    "            score_dicts['num_tokens_scored'] = score_dict['num_tokens_scored']\n",
    "            score_dicts['num_green_tokens'] = score_dict['num_green_tokens']\n",
    "\n",
    "            res.append(score_dicts)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_z_beam(5, gammas=[0.1, 0.25, 0.5, 0.75, 0.9], num_beams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gamma': 0.1,\n",
       "  'token_size': 50,\n",
       "  'delta': 5,\n",
       "  'z_score': 5.196152422706631,\n",
       "  'p_value': 1.017277307272218e-07,\n",
       "  'num_tokens_scored': 363,\n",
       "  'num_green_tokens': 66},\n",
       " {'gamma': 0.25,\n",
       "  'token_size': 50,\n",
       "  'delta': 5,\n",
       "  'z_score': 2.8181818181818183,\n",
       "  'p_value': 0.0024148226279699673,\n",
       "  'num_tokens_scored': 363,\n",
       "  'num_green_tokens': 114},\n",
       " {'gamma': 0.5,\n",
       "  'token_size': 50,\n",
       "  'delta': 5,\n",
       "  'z_score': 3.306642450813311,\n",
       "  'p_value': 0.00047210664009389826,\n",
       "  'num_tokens_scored': 363,\n",
       "  'num_green_tokens': 213},\n",
       " {'gamma': 0.75,\n",
       "  'token_size': 50,\n",
       "  'delta': 5,\n",
       "  'z_score': 1.303030303030303,\n",
       "  'p_value': 0.09628220813361954,\n",
       "  'num_tokens_scored': 363,\n",
       "  'num_green_tokens': 283},\n",
       " {'gamma': 0.9,\n",
       "  'token_size': 50,\n",
       "  'delta': 5,\n",
       "  'z_score': 0.9272595232439466,\n",
       "  'p_value': 0.176895902320312,\n",
       "  'num_tokens_scored': 363,\n",
       "  'num_green_tokens': 332},\n",
       " {'gamma': 0.1,\n",
       "  'token_size': 100,\n",
       "  'delta': 5,\n",
       "  'z_score': 12.416511609932599,\n",
       "  'p_value': 1.0631624192359956e-35,\n",
       "  'num_tokens_scored': 413,\n",
       "  'num_green_tokens': 117},\n",
       " {'gamma': 0.25,\n",
       "  'token_size': 100,\n",
       "  'delta': 5,\n",
       "  'z_score': 6.903520525184703,\n",
       "  'p_value': 2.5364661341575887e-12,\n",
       "  'num_tokens_scored': 413,\n",
       "  'num_green_tokens': 164},\n",
       " {'gamma': 0.5,\n",
       "  'token_size': 100,\n",
       "  'delta': 5,\n",
       "  'z_score': 5.560366493747889,\n",
       "  'p_value': 1.3460436109706446e-08,\n",
       "  'num_tokens_scored': 413,\n",
       "  'num_green_tokens': 263},\n",
       " {'gamma': 0.75,\n",
       "  'token_size': 100,\n",
       "  'delta': 5,\n",
       "  'z_score': 2.6168485703474413,\n",
       "  'p_value': 0.004437285101670033,\n",
       "  'num_tokens_scored': 412,\n",
       "  'num_green_tokens': 332},\n",
       " {'gamma': 0.9,\n",
       "  'token_size': 100,\n",
       "  'delta': 5,\n",
       "  'z_score': 1.6894328874809241,\n",
       "  'p_value': 0.04556825148985418,\n",
       "  'num_tokens_scored': 413,\n",
       "  'num_green_tokens': 382},\n",
       " {'gamma': 0.1,\n",
       "  'token_size': 150,\n",
       "  'delta': 5,\n",
       "  'z_score': 18.698015689819627,\n",
       "  'p_value': 2.5690747999162926e-78,\n",
       "  'num_tokens_scored': 463,\n",
       "  'num_green_tokens': 167},\n",
       " {'gamma': 0.25,\n",
       "  'token_size': 150,\n",
       "  'delta': 5,\n",
       "  'z_score': 10.544874795332758,\n",
       "  'p_value': 2.682213051433142e-26,\n",
       "  'num_tokens_scored': 463,\n",
       "  'num_green_tokens': 214},\n",
       " {'gamma': 0.5,\n",
       "  'token_size': 150,\n",
       "  'delta': 5,\n",
       "  'z_score': 7.575252421144821,\n",
       "  'p_value': 1.7921501683638172e-14,\n",
       "  'num_tokens_scored': 463,\n",
       "  'num_green_tokens': 313},\n",
       " {'gamma': 0.75,\n",
       "  'token_size': 150,\n",
       "  'delta': 5,\n",
       "  'z_score': 3.836939174892072,\n",
       "  'p_value': 6.228865583899642e-05,\n",
       "  'num_tokens_scored': 463,\n",
       "  'num_green_tokens': 383},\n",
       " {'gamma': 0.9,\n",
       "  'token_size': 150,\n",
       "  'delta': 5,\n",
       "  'z_score': 2.2152578654881605,\n",
       "  'p_value': 0.013371185056971428,\n",
       "  'num_tokens_scored': 463,\n",
       "  'num_green_tokens': 431},\n",
       " {'gamma': 0.1,\n",
       "  'token_size': 200,\n",
       "  'delta': 5,\n",
       "  'z_score': 24.386112396186416,\n",
       "  'p_value': 1.2005519204027113e-131,\n",
       "  'num_tokens_scored': 513,\n",
       "  'num_green_tokens': 217},\n",
       " {'gamma': 0.25,\n",
       "  'token_size': 200,\n",
       "  'delta': 5,\n",
       "  'z_score': 13.841415943523891,\n",
       "  'p_value': 7.167733125196067e-44,\n",
       "  'num_tokens_scored': 513,\n",
       "  'num_green_tokens': 264},\n",
       " {'gamma': 0.5,\n",
       "  'token_size': 200,\n",
       "  'delta': 5,\n",
       "  'z_score': 9.404179735161811,\n",
       "  'p_value': 2.621880387387806e-21,\n",
       "  'num_tokens_scored': 513,\n",
       "  'num_green_tokens': 363},\n",
       " {'gamma': 0.75,\n",
       "  'token_size': 200,\n",
       "  'delta': 5,\n",
       "  'z_score': 4.919692959668713,\n",
       "  'p_value': 4.33400391533067e-07,\n",
       "  'num_tokens_scored': 513,\n",
       "  'num_green_tokens': 433},\n",
       " {'gamma': 0.9,\n",
       "  'token_size': 200,\n",
       "  'delta': 5,\n",
       "  'z_score': 2.84038605459504,\n",
       "  'p_value': 0.0022529483550174927,\n",
       "  'num_tokens_scored': 513,\n",
       "  'num_green_tokens': 481}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./fig3_beams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the dependence of Z score on green list size parameter (gamma) with constant delta using a 4 way beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the dependence of Z score on green list size (gamma) using multinomial sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_z_sampling(delta, gammas):\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for max_new_tokens in range(50, 201, 50):\n",
    "\n",
    "        gen_kwargs = dict(max_new_tokens=max_new_tokens)\n",
    "\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample = True, \n",
    "            top_k = 0,\n",
    "            temperature = 0.7\n",
    "         ))\n",
    "\n",
    "        for gamma in gammas:\n",
    "            score_dicts = {}\n",
    "            \n",
    "            decoded_watermark = generate_watermark(gamma=gamma, delta=delta, gen_kwargs=gen_kwargs)\n",
    "            score_dict = detect_watermark(decoded_watermark, gamma, delta)\n",
    "\n",
    "            score_dicts['gamma'] = gamma\n",
    "            score_dicts['token_size'] = max_new_tokens\n",
    "            score_dicts['delta'] = delta\n",
    "            score_dicts['z_score'] = score_dict['z_score']\n",
    "            score_dicts['p_value'] = score_dict['p_value']\n",
    "            score_dicts['num_tokens_scored'] = score_dict['num_tokens_scored']\n",
    "            score_dicts['num_green_tokens'] = score_dict['num_green_tokens']\n",
    "\n",
    "            res.append(score_dicts)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_z_sampling(delta=5, gammas=[0.1, 0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "df.to_csv('./fig3_sampling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
